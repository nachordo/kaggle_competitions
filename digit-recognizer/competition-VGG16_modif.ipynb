{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import keras,os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Images\n",
    "raw=pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41997</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41999</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42000 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0          1       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          1       0       0       0       0       0       0       0       0   \n",
       "3          4       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "41995      0       0       0       0       0       0       0       0       0   \n",
       "41996      1       0       0       0       0       0       0       0       0   \n",
       "41997      7       0       0       0       0       0       0       0       0   \n",
       "41998      6       0       0       0       0       0       0       0       0   \n",
       "41999      9       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0           0  ...         0         0         0         0         0   \n",
       "1           0  ...         0         0         0         0         0   \n",
       "2           0  ...         0         0         0         0         0   \n",
       "3           0  ...         0         0         0         0         0   \n",
       "4           0  ...         0         0         0         0         0   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "41995       0  ...         0         0         0         0         0   \n",
       "41996       0  ...         0         0         0         0         0   \n",
       "41997       0  ...         0         0         0         0         0   \n",
       "41998       0  ...         0         0         0         0         0   \n",
       "41999       0  ...         0         0         0         0         0   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0             0         0         0         0         0  \n",
       "1             0         0         0         0         0  \n",
       "2             0         0         0         0         0  \n",
       "3             0         0         0         0         0  \n",
       "4             0         0         0         0         0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "41995         0         0         0         0         0  \n",
       "41996         0         0         0         0         0  \n",
       "41997         0         0         0         0         0  \n",
       "41998         0         0         0         0         0  \n",
       "41999         0         0         0         0         0  \n",
       "\n",
       "[42000 rows x 785 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid=raw.drop([\"label\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical \n",
    "\n",
    "y=to_categorical(raw[\"label\"].values, num_classes = 10)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_valid.values.reshape(42000, 28, 28)/ 255.0\n",
    "X = X.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 28, 28, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[-1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 28, 28, 1), (None, 10)), types: (tf.float64, tf.float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset=tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(1000)\n",
    "valid_dataset=tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(1000)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=(28,28,1),filters=16,kernel_size=(2,2),padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=16,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2), padding='same',strides=(2,2)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2), padding='same',strides=(2,2)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2), padding='same',strides=(2,2)))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2), padding='same',strides=(2,2)))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2), padding='same',strides=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1024,activation=\"relu\"))\n",
    "model.add(Dense(units=1024,activation=\"relu\"))\n",
    "model.add(Dense(units=10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 16)        80        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 128)         147584    \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 2, 2, 128)         147584    \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 128)         147584    \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              132096    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 2,112,362\n",
      "Trainable params: 2,112,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpoint = ModelCheckpoint('VGG16_modif_best.h5', verbose=1, monitor='val_accuracy', save_best_only=True, mode='auto')\n",
    "early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, verbose=1, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.2922 - accuracy: 0.1099\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.11143, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 16s 805ms/step - loss: 2.2922 - accuracy: 0.1099 - val_loss: 2.2794 - val_accuracy: 0.1114\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.2967 - accuracy: 0.1003\n",
      "Epoch 00002: val_accuracy improved from 0.11143 to 0.15429, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 15s 773ms/step - loss: 2.2967 - accuracy: 0.1003 - val_loss: 2.2757 - val_accuracy: 0.1543\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.2512 - accuracy: 0.1419\n",
      "Epoch 00003: val_accuracy did not improve from 0.15429\n",
      "20/20 [==============================] - 16s 807ms/step - loss: 2.2512 - accuracy: 0.1419 - val_loss: 2.1240 - val_accuracy: 0.1114\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.0529 - accuracy: 0.2058\n",
      "Epoch 00004: val_accuracy improved from 0.15429 to 0.24560, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 16s 789ms/step - loss: 2.0529 - accuracy: 0.2058 - val_loss: 1.8448 - val_accuracy: 0.2456\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.7610 - accuracy: 0.2765\n",
      "Epoch 00005: val_accuracy improved from 0.24560 to 0.42893, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 15s 771ms/step - loss: 1.7610 - accuracy: 0.2765 - val_loss: 1.6913 - val_accuracy: 0.4289\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.5069 - accuracy: 0.4133\n",
      "Epoch 00006: val_accuracy did not improve from 0.42893\n",
      "20/20 [==============================] - 15s 769ms/step - loss: 1.5069 - accuracy: 0.4133 - val_loss: 1.6895 - val_accuracy: 0.3069\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.3361 - accuracy: 0.4779\n",
      "Epoch 00007: val_accuracy improved from 0.42893 to 0.53286, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 16s 785ms/step - loss: 1.3361 - accuracy: 0.4779 - val_loss: 1.2654 - val_accuracy: 0.5329\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.0611 - accuracy: 0.6011\n",
      "Epoch 00008: val_accuracy improved from 0.53286 to 0.63250, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 17s 855ms/step - loss: 1.0611 - accuracy: 0.6011 - val_loss: 0.9601 - val_accuracy: 0.6325\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.8517 - accuracy: 0.6845\n",
      "Epoch 00009: val_accuracy improved from 0.63250 to 0.73702, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 15s 771ms/step - loss: 0.8517 - accuracy: 0.6845 - val_loss: 0.7213 - val_accuracy: 0.7370\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.5470 - accuracy: 0.7988\n",
      "Epoch 00010: val_accuracy improved from 0.73702 to 0.83643, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 17s 837ms/step - loss: 0.5470 - accuracy: 0.7988 - val_loss: 0.4419 - val_accuracy: 0.8364\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.3782 - accuracy: 0.8667\n",
      "Epoch 00011: val_accuracy improved from 0.83643 to 0.88179, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 16s 782ms/step - loss: 0.3782 - accuracy: 0.8667 - val_loss: 0.3586 - val_accuracy: 0.8818\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.2810 - accuracy: 0.9118\n",
      "Epoch 00012: val_accuracy improved from 0.88179 to 0.93821, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 16s 783ms/step - loss: 0.2810 - accuracy: 0.9118 - val_loss: 0.2012 - val_accuracy: 0.9382\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.2061 - accuracy: 0.9402\n",
      "Epoch 00013: val_accuracy improved from 0.93821 to 0.95369, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 17s 847ms/step - loss: 0.2061 - accuracy: 0.9402 - val_loss: 0.1514 - val_accuracy: 0.9537\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1438 - accuracy: 0.9582\n",
      "Epoch 00014: val_accuracy improved from 0.95369 to 0.96214, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 17s 864ms/step - loss: 0.1438 - accuracy: 0.9582 - val_loss: 0.1235 - val_accuracy: 0.9621\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9654\n",
      "Epoch 00015: val_accuracy improved from 0.96214 to 0.96643, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 19s 964ms/step - loss: 0.1211 - accuracy: 0.9654 - val_loss: 0.1104 - val_accuracy: 0.9664\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9710\n",
      "Epoch 00016: val_accuracy improved from 0.96643 to 0.96869, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 17s 864ms/step - loss: 0.1043 - accuracy: 0.9710 - val_loss: 0.1049 - val_accuracy: 0.9687\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9705\n",
      "Epoch 00017: val_accuracy did not improve from 0.96869\n",
      "20/20 [==============================] - 16s 798ms/step - loss: 0.1005 - accuracy: 0.9705 - val_loss: 0.1040 - val_accuracy: 0.9679\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1051 - accuracy: 0.9679\n",
      "Epoch 00018: val_accuracy improved from 0.96869 to 0.96905, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 16s 822ms/step - loss: 0.1051 - accuracy: 0.9679 - val_loss: 0.1022 - val_accuracy: 0.9690\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0908 - accuracy: 0.9729\n",
      "Epoch 00019: val_accuracy improved from 0.96905 to 0.97405, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 17s 833ms/step - loss: 0.0908 - accuracy: 0.9729 - val_loss: 0.0897 - val_accuracy: 0.9740\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0872 - accuracy: 0.9741\n",
      "Epoch 00020: val_accuracy improved from 0.97405 to 0.97476, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 16s 813ms/step - loss: 0.0872 - accuracy: 0.9741 - val_loss: 0.0794 - val_accuracy: 0.9748\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9753\n",
      "Epoch 00021: val_accuracy did not improve from 0.97476\n",
      "20/20 [==============================] - 17s 833ms/step - loss: 0.0802 - accuracy: 0.9753 - val_loss: 0.0913 - val_accuracy: 0.9726\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9786\n",
      "Epoch 00022: val_accuracy improved from 0.97476 to 0.97762, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 16s 798ms/step - loss: 0.0755 - accuracy: 0.9786 - val_loss: 0.0698 - val_accuracy: 0.9776\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9803\n",
      "Epoch 00023: val_accuracy did not improve from 0.97762\n",
      "20/20 [==============================] - 15s 771ms/step - loss: 0.0664 - accuracy: 0.9803 - val_loss: 0.0869 - val_accuracy: 0.9735\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9795\n",
      "Epoch 00024: val_accuracy did not improve from 0.97762\n",
      "20/20 [==============================] - 16s 786ms/step - loss: 0.0686 - accuracy: 0.9795 - val_loss: 0.0904 - val_accuracy: 0.9724\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9762\n",
      "Epoch 00025: val_accuracy did not improve from 0.97762\n",
      "20/20 [==============================] - 16s 800ms/step - loss: 0.0784 - accuracy: 0.9762 - val_loss: 0.0791 - val_accuracy: 0.9744\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9825\n",
      "Epoch 00026: val_accuracy improved from 0.97762 to 0.97917, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 16s 800ms/step - loss: 0.0612 - accuracy: 0.9825 - val_loss: 0.0745 - val_accuracy: 0.9792\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9841\n",
      "Epoch 00027: val_accuracy did not improve from 0.97917\n",
      "20/20 [==============================] - 16s 812ms/step - loss: 0.0527 - accuracy: 0.9841 - val_loss: 0.0697 - val_accuracy: 0.9788\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9828\n",
      "Epoch 00028: val_accuracy did not improve from 0.97917\n",
      "20/20 [==============================] - 16s 793ms/step - loss: 0.0518 - accuracy: 0.9828 - val_loss: 0.0708 - val_accuracy: 0.9785\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9848\n",
      "Epoch 00029: val_accuracy did not improve from 0.97917\n",
      "20/20 [==============================] - 16s 821ms/step - loss: 0.0497 - accuracy: 0.9848 - val_loss: 0.0718 - val_accuracy: 0.9782\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9843\n",
      "Epoch 00030: val_accuracy did not improve from 0.97917\n",
      "20/20 [==============================] - 19s 961ms/step - loss: 0.0487 - accuracy: 0.9843 - val_loss: 0.0677 - val_accuracy: 0.9792\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9771\n",
      "Epoch 00031: val_accuracy did not improve from 0.97917\n",
      "20/20 [==============================] - 17s 843ms/step - loss: 0.0730 - accuracy: 0.9771 - val_loss: 0.0794 - val_accuracy: 0.9761\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.9837\n",
      "Epoch 00032: val_accuracy improved from 0.97917 to 0.98012, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 18s 901ms/step - loss: 0.0580 - accuracy: 0.9837 - val_loss: 0.0686 - val_accuracy: 0.9801\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9843\n",
      "Epoch 00033: val_accuracy did not improve from 0.98012\n",
      "20/20 [==============================] - 18s 876ms/step - loss: 0.0521 - accuracy: 0.9843 - val_loss: 0.0789 - val_accuracy: 0.9762\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9845\n",
      "Epoch 00034: val_accuracy did not improve from 0.98012\n",
      "20/20 [==============================] - 16s 803ms/step - loss: 0.0514 - accuracy: 0.9845 - val_loss: 0.0711 - val_accuracy: 0.9794\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9868\n",
      "Epoch 00035: val_accuracy improved from 0.98012 to 0.98167, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 17s 849ms/step - loss: 0.0419 - accuracy: 0.9868 - val_loss: 0.0596 - val_accuracy: 0.9817\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9855\n",
      "Epoch 00036: val_accuracy did not improve from 0.98167\n",
      "20/20 [==============================] - 16s 819ms/step - loss: 0.0460 - accuracy: 0.9855 - val_loss: 0.0659 - val_accuracy: 0.9810\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9865\n",
      "Epoch 00037: val_accuracy improved from 0.98167 to 0.98202, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 16s 815ms/step - loss: 0.0442 - accuracy: 0.9865 - val_loss: 0.0655 - val_accuracy: 0.9820\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9883\n",
      "Epoch 00038: val_accuracy did not improve from 0.98202\n",
      "20/20 [==============================] - 17s 845ms/step - loss: 0.0388 - accuracy: 0.9883 - val_loss: 0.0670 - val_accuracy: 0.9808\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9858\n",
      "Epoch 00039: val_accuracy did not improve from 0.98202\n",
      "20/20 [==============================] - 16s 820ms/step - loss: 0.0465 - accuracy: 0.9858 - val_loss: 0.0819 - val_accuracy: 0.9779\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9866\n",
      "Epoch 00040: val_accuracy did not improve from 0.98202\n",
      "20/20 [==============================] - 16s 806ms/step - loss: 0.0428 - accuracy: 0.9866 - val_loss: 0.0720 - val_accuracy: 0.9820\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9865\n",
      "Epoch 00041: val_accuracy did not improve from 0.98202\n",
      "20/20 [==============================] - 16s 809ms/step - loss: 0.0429 - accuracy: 0.9865 - val_loss: 0.0822 - val_accuracy: 0.9781\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9864\n",
      "Epoch 00042: val_accuracy did not improve from 0.98202\n",
      "20/20 [==============================] - 17s 856ms/step - loss: 0.0439 - accuracy: 0.9864 - val_loss: 0.0681 - val_accuracy: 0.9814\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9864\n",
      "Epoch 00043: val_accuracy did not improve from 0.98202\n",
      "20/20 [==============================] - 17s 828ms/step - loss: 0.0433 - accuracy: 0.9864 - val_loss: 0.0691 - val_accuracy: 0.9811\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9858\n",
      "Epoch 00044: val_accuracy improved from 0.98202 to 0.98238, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 17s 835ms/step - loss: 0.0471 - accuracy: 0.9858 - val_loss: 0.0672 - val_accuracy: 0.9824\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9879\n",
      "Epoch 00045: val_accuracy improved from 0.98238 to 0.98512, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 16s 819ms/step - loss: 0.0411 - accuracy: 0.9879 - val_loss: 0.0576 - val_accuracy: 0.9851\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9855\n",
      "Epoch 00046: val_accuracy did not improve from 0.98512\n",
      "20/20 [==============================] - 16s 801ms/step - loss: 0.0506 - accuracy: 0.9855 - val_loss: 0.0557 - val_accuracy: 0.9842\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9875\n",
      "Epoch 00047: val_accuracy did not improve from 0.98512\n",
      "20/20 [==============================] - 16s 813ms/step - loss: 0.0434 - accuracy: 0.9875 - val_loss: 0.0689 - val_accuracy: 0.9819\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9856\n",
      "Epoch 00048: val_accuracy did not improve from 0.98512\n",
      "20/20 [==============================] - 16s 800ms/step - loss: 0.0554 - accuracy: 0.9856 - val_loss: 0.0795 - val_accuracy: 0.9793\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9869\n",
      "Epoch 00049: val_accuracy did not improve from 0.98512\n",
      "20/20 [==============================] - 16s 820ms/step - loss: 0.0453 - accuracy: 0.9869 - val_loss: 0.0659 - val_accuracy: 0.9811\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9893\n",
      "Epoch 00050: val_accuracy did not improve from 0.98512\n",
      "20/20 [==============================] - 17s 828ms/step - loss: 0.0357 - accuracy: 0.9893 - val_loss: 0.0631 - val_accuracy: 0.9849\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9909\n",
      "Epoch 00051: val_accuracy did not improve from 0.98512\n",
      "20/20 [==============================] - 16s 803ms/step - loss: 0.0302 - accuracy: 0.9909 - val_loss: 0.0525 - val_accuracy: 0.9845\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9907\n",
      "Epoch 00052: val_accuracy did not improve from 0.98512\n",
      "20/20 [==============================] - 16s 816ms/step - loss: 0.0304 - accuracy: 0.9907 - val_loss: 0.0631 - val_accuracy: 0.9821\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9886\n",
      "Epoch 00053: val_accuracy did not improve from 0.98512\n",
      "20/20 [==============================] - 15s 774ms/step - loss: 0.0353 - accuracy: 0.9886 - val_loss: 0.0612 - val_accuracy: 0.9823\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9887\n",
      "Epoch 00054: val_accuracy did not improve from 0.98512\n",
      "20/20 [==============================] - 16s 818ms/step - loss: 0.0357 - accuracy: 0.9887 - val_loss: 0.0709 - val_accuracy: 0.9832\n",
      "Epoch 55/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9856\n",
      "Epoch 00055: val_accuracy did not improve from 0.98512\n",
      "20/20 [==============================] - 16s 799ms/step - loss: 0.0484 - accuracy: 0.9856 - val_loss: 0.0637 - val_accuracy: 0.9831\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9877\n",
      "Epoch 00056: val_accuracy improved from 0.98512 to 0.98607, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 17s 842ms/step - loss: 0.0424 - accuracy: 0.9877 - val_loss: 0.0544 - val_accuracy: 0.9861\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9892\n",
      "Epoch 00057: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 16s 785ms/step - loss: 0.0387 - accuracy: 0.9892 - val_loss: 0.0992 - val_accuracy: 0.9763\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.9810\n",
      "Epoch 00058: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 16s 784ms/step - loss: 0.0643 - accuracy: 0.9810 - val_loss: 0.0798 - val_accuracy: 0.9800\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9864\n",
      "Epoch 00059: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 16s 811ms/step - loss: 0.0502 - accuracy: 0.9864 - val_loss: 0.0748 - val_accuracy: 0.9812\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0499 - accuracy: 0.9870\n",
      "Epoch 00060: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 17s 836ms/step - loss: 0.0499 - accuracy: 0.9870 - val_loss: 0.0776 - val_accuracy: 0.9849\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 0.9890\n",
      "Epoch 00061: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 16s 812ms/step - loss: 0.0423 - accuracy: 0.9890 - val_loss: 0.0642 - val_accuracy: 0.9851\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9915\n",
      "Epoch 00062: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 16s 781ms/step - loss: 0.0345 - accuracy: 0.9915 - val_loss: 0.0826 - val_accuracy: 0.9827\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9885\n",
      "Epoch 00063: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 16s 804ms/step - loss: 0.0416 - accuracy: 0.9885 - val_loss: 0.0636 - val_accuracy: 0.9843\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9923\n",
      "Epoch 00064: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 16s 800ms/step - loss: 0.0259 - accuracy: 0.9923 - val_loss: 0.0698 - val_accuracy: 0.9833\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9893\n",
      "Epoch 00065: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 16s 778ms/step - loss: 0.0404 - accuracy: 0.9893 - val_loss: 0.0793 - val_accuracy: 0.9812\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9902\n",
      "Epoch 00066: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 17s 832ms/step - loss: 0.0342 - accuracy: 0.9902 - val_loss: 0.0697 - val_accuracy: 0.9829\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9926\n",
      "Epoch 00067: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 15s 772ms/step - loss: 0.0256 - accuracy: 0.9926 - val_loss: 0.0793 - val_accuracy: 0.9826\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9897\n",
      "Epoch 00068: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 17s 836ms/step - loss: 0.0353 - accuracy: 0.9897 - val_loss: 0.1235 - val_accuracy: 0.9669\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 0.9854\n",
      "Epoch 00069: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 16s 799ms/step - loss: 0.0552 - accuracy: 0.9854 - val_loss: 0.0529 - val_accuracy: 0.9852\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9912\n",
      "Epoch 00070: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 16s 792ms/step - loss: 0.0328 - accuracy: 0.9912 - val_loss: 0.0650 - val_accuracy: 0.9851\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9920\n",
      "Epoch 00071: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 16s 808ms/step - loss: 0.0290 - accuracy: 0.9920 - val_loss: 0.0695 - val_accuracy: 0.9823\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9903\n",
      "Epoch 00072: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 16s 793ms/step - loss: 0.0340 - accuracy: 0.9903 - val_loss: 0.0901 - val_accuracy: 0.9771\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9915\n",
      "Epoch 00073: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 17s 841ms/step - loss: 0.0293 - accuracy: 0.9915 - val_loss: 0.0732 - val_accuracy: 0.9848\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9921\n",
      "Epoch 00074: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 16s 815ms/step - loss: 0.0248 - accuracy: 0.9921 - val_loss: 0.1028 - val_accuracy: 0.9780\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9907\n",
      "Epoch 00075: val_accuracy did not improve from 0.98607\n",
      "20/20 [==============================] - 15s 763ms/step - loss: 0.0328 - accuracy: 0.9907 - val_loss: 0.0782 - val_accuracy: 0.9833\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9920\n",
      "Epoch 00076: val_accuracy improved from 0.98607 to 0.98726, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 16s 815ms/step - loss: 0.0265 - accuracy: 0.9920 - val_loss: 0.0610 - val_accuracy: 0.9873\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9919\n",
      "Epoch 00077: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 802ms/step - loss: 0.0249 - accuracy: 0.9919 - val_loss: 0.0734 - val_accuracy: 0.9842\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9919\n",
      "Epoch 00078: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 19s 966ms/step - loss: 0.0313 - accuracy: 0.9919 - val_loss: 0.0639 - val_accuracy: 0.9857\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9915\n",
      "Epoch 00079: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 17s 835ms/step - loss: 0.0262 - accuracy: 0.9915 - val_loss: 0.0562 - val_accuracy: 0.9873\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9938\n",
      "Epoch 00080: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 822ms/step - loss: 0.0218 - accuracy: 0.9938 - val_loss: 0.0808 - val_accuracy: 0.9855\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9898\n",
      "Epoch 00081: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 798ms/step - loss: 0.0327 - accuracy: 0.9898 - val_loss: 0.0733 - val_accuracy: 0.9801\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9930\n",
      "Epoch 00082: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 809ms/step - loss: 0.0255 - accuracy: 0.9930 - val_loss: 0.0731 - val_accuracy: 0.9862\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9883\n",
      "Epoch 00083: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 811ms/step - loss: 0.0474 - accuracy: 0.9883 - val_loss: 0.0868 - val_accuracy: 0.9810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9906\n",
      "Epoch 00084: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 787ms/step - loss: 0.0344 - accuracy: 0.9906 - val_loss: 0.0655 - val_accuracy: 0.9854\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9923\n",
      "Epoch 00085: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 15s 765ms/step - loss: 0.0257 - accuracy: 0.9923 - val_loss: 0.0979 - val_accuracy: 0.9832\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9904\n",
      "Epoch 00086: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 17s 851ms/step - loss: 0.0375 - accuracy: 0.9904 - val_loss: 0.0647 - val_accuracy: 0.9835\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9911\n",
      "Epoch 00087: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 814ms/step - loss: 0.0317 - accuracy: 0.9911 - val_loss: 0.0816 - val_accuracy: 0.9830\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9921\n",
      "Epoch 00088: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 797ms/step - loss: 0.0267 - accuracy: 0.9921 - val_loss: 0.0641 - val_accuracy: 0.9868\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9924\n",
      "Epoch 00089: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 811ms/step - loss: 0.0238 - accuracy: 0.9924 - val_loss: 0.0770 - val_accuracy: 0.9849\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9906\n",
      "Epoch 00090: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 797ms/step - loss: 0.0315 - accuracy: 0.9906 - val_loss: 0.0802 - val_accuracy: 0.9829\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9915\n",
      "Epoch 00091: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 804ms/step - loss: 0.0317 - accuracy: 0.9915 - val_loss: 0.0713 - val_accuracy: 0.9824\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9917\n",
      "Epoch 00092: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 803ms/step - loss: 0.0279 - accuracy: 0.9917 - val_loss: 0.1027 - val_accuracy: 0.9833\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9919\n",
      "Epoch 00093: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 790ms/step - loss: 0.0297 - accuracy: 0.9919 - val_loss: 0.0687 - val_accuracy: 0.9855\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9930\n",
      "Epoch 00094: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 821ms/step - loss: 0.0241 - accuracy: 0.9930 - val_loss: 0.0663 - val_accuracy: 0.9855\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9937\n",
      "Epoch 00095: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 17s 835ms/step - loss: 0.0197 - accuracy: 0.9937 - val_loss: 0.0889 - val_accuracy: 0.9861\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9954\n",
      "Epoch 00096: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 798ms/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.0660 - val_accuracy: 0.9837\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9933\n",
      "Epoch 00097: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 815ms/step - loss: 0.0219 - accuracy: 0.9933 - val_loss: 0.0704 - val_accuracy: 0.9843\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9919\n",
      "Epoch 00098: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 823ms/step - loss: 0.0289 - accuracy: 0.9919 - val_loss: 0.0918 - val_accuracy: 0.9808\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9919\n",
      "Epoch 00099: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 796ms/step - loss: 0.0297 - accuracy: 0.9919 - val_loss: 0.0754 - val_accuracy: 0.9839\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9920\n",
      "Epoch 00100: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 803ms/step - loss: 0.0286 - accuracy: 0.9920 - val_loss: 0.0985 - val_accuracy: 0.9835\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9929\n",
      "Epoch 00101: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 18s 889ms/step - loss: 0.0285 - accuracy: 0.9929 - val_loss: 0.0900 - val_accuracy: 0.9842\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9915\n",
      "Epoch 00102: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 798ms/step - loss: 0.0320 - accuracy: 0.9915 - val_loss: 0.0889 - val_accuracy: 0.9850\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9928\n",
      "Epoch 00103: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 17s 825ms/step - loss: 0.0261 - accuracy: 0.9928 - val_loss: 0.0912 - val_accuracy: 0.9844\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9913\n",
      "Epoch 00104: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 15s 771ms/step - loss: 0.0319 - accuracy: 0.9913 - val_loss: 0.0767 - val_accuracy: 0.9826\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0504 - accuracy: 0.9869\n",
      "Epoch 00105: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 795ms/step - loss: 0.0504 - accuracy: 0.9869 - val_loss: 0.0735 - val_accuracy: 0.9869\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9919\n",
      "Epoch 00106: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 17s 832ms/step - loss: 0.0328 - accuracy: 0.9919 - val_loss: 0.0720 - val_accuracy: 0.9843\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9944\n",
      "Epoch 00107: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 788ms/step - loss: 0.0211 - accuracy: 0.9944 - val_loss: 0.0824 - val_accuracy: 0.9863\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9947\n",
      "Epoch 00108: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 822ms/step - loss: 0.0233 - accuracy: 0.9947 - val_loss: 0.0703 - val_accuracy: 0.9849\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9902\n",
      "Epoch 00109: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 790ms/step - loss: 0.0395 - accuracy: 0.9902 - val_loss: 0.0643 - val_accuracy: 0.9860\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9922\n",
      "Epoch 00110: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 17s 833ms/step - loss: 0.0293 - accuracy: 0.9922 - val_loss: 0.0763 - val_accuracy: 0.9862\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9904\n",
      "Epoch 00111: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 814ms/step - loss: 0.0383 - accuracy: 0.9904 - val_loss: 0.0899 - val_accuracy: 0.9838\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9898\n",
      "Epoch 00112: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 823ms/step - loss: 0.0407 - accuracy: 0.9898 - val_loss: 0.0600 - val_accuracy: 0.9854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9928\n",
      "Epoch 00113: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 808ms/step - loss: 0.0250 - accuracy: 0.9928 - val_loss: 0.0828 - val_accuracy: 0.9864\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9936\n",
      "Epoch 00114: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 812ms/step - loss: 0.0247 - accuracy: 0.9936 - val_loss: 0.0835 - val_accuracy: 0.9837\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9922\n",
      "Epoch 00115: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 18s 893ms/step - loss: 0.0290 - accuracy: 0.9922 - val_loss: 0.0864 - val_accuracy: 0.9849\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9815\n",
      "Epoch 00116: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 808ms/step - loss: 0.0760 - accuracy: 0.9815 - val_loss: 0.1120 - val_accuracy: 0.9742\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9820\n",
      "Epoch 00117: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 810ms/step - loss: 0.0730 - accuracy: 0.9820 - val_loss: 0.0876 - val_accuracy: 0.9806\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9891\n",
      "Epoch 00118: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 802ms/step - loss: 0.0411 - accuracy: 0.9891 - val_loss: 0.0939 - val_accuracy: 0.9826\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9900\n",
      "Epoch 00119: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 778ms/step - loss: 0.0435 - accuracy: 0.9900 - val_loss: 0.0752 - val_accuracy: 0.9814\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 0.9882\n",
      "Epoch 00120: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 17s 853ms/step - loss: 0.0446 - accuracy: 0.9882 - val_loss: 0.0677 - val_accuracy: 0.9842\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9885\n",
      "Epoch 00121: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 788ms/step - loss: 0.0473 - accuracy: 0.9885 - val_loss: 0.0958 - val_accuracy: 0.9798\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9891\n",
      "Epoch 00122: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 802ms/step - loss: 0.0390 - accuracy: 0.9891 - val_loss: 0.0914 - val_accuracy: 0.9846\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9926\n",
      "Epoch 00123: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 15s 765ms/step - loss: 0.0284 - accuracy: 0.9926 - val_loss: 0.0760 - val_accuracy: 0.9869\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9949\n",
      "Epoch 00124: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 793ms/step - loss: 0.0194 - accuracy: 0.9949 - val_loss: 0.1135 - val_accuracy: 0.9843\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9905\n",
      "Epoch 00125: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 803ms/step - loss: 0.0355 - accuracy: 0.9905 - val_loss: 0.0817 - val_accuracy: 0.9837\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9928\n",
      "Epoch 00126: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 801ms/step - loss: 0.0266 - accuracy: 0.9928 - val_loss: 0.0609 - val_accuracy: 0.9858\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9936\n",
      "Epoch 00127: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 811ms/step - loss: 0.0233 - accuracy: 0.9936 - val_loss: 0.0789 - val_accuracy: 0.9863\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9952\n",
      "Epoch 00128: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 782ms/step - loss: 0.0161 - accuracy: 0.9952 - val_loss: 0.1146 - val_accuracy: 0.9849\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9941\n",
      "Epoch 00129: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 817ms/step - loss: 0.0255 - accuracy: 0.9941 - val_loss: 0.0799 - val_accuracy: 0.9840\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9947\n",
      "Epoch 00130: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 17s 829ms/step - loss: 0.0204 - accuracy: 0.9947 - val_loss: 0.0820 - val_accuracy: 0.9863\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9924\n",
      "Epoch 00131: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 17s 834ms/step - loss: 0.0298 - accuracy: 0.9924 - val_loss: 0.0747 - val_accuracy: 0.9848\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9940\n",
      "Epoch 00132: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 789ms/step - loss: 0.0236 - accuracy: 0.9940 - val_loss: 0.0817 - val_accuracy: 0.9860\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9930\n",
      "Epoch 00133: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 805ms/step - loss: 0.0348 - accuracy: 0.9930 - val_loss: 0.1004 - val_accuracy: 0.9788\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9911\n",
      "Epoch 00134: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 17s 846ms/step - loss: 0.0351 - accuracy: 0.9911 - val_loss: 0.0855 - val_accuracy: 0.9838\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9922\n",
      "Epoch 00135: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 808ms/step - loss: 0.0315 - accuracy: 0.9922 - val_loss: 0.0878 - val_accuracy: 0.9807\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9932\n",
      "Epoch 00136: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 801ms/step - loss: 0.0240 - accuracy: 0.9932 - val_loss: 0.0752 - val_accuracy: 0.9864\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9931\n",
      "Epoch 00137: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 17s 831ms/step - loss: 0.0294 - accuracy: 0.9931 - val_loss: 0.0853 - val_accuracy: 0.9788\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9912\n",
      "Epoch 00138: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 783ms/step - loss: 0.0317 - accuracy: 0.9912 - val_loss: 0.0812 - val_accuracy: 0.9852\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9926\n",
      "Epoch 00139: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 813ms/step - loss: 0.0308 - accuracy: 0.9926 - val_loss: 0.0832 - val_accuracy: 0.9852\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9928\n",
      "Epoch 00140: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 17s 826ms/step - loss: 0.0301 - accuracy: 0.9928 - val_loss: 0.0948 - val_accuracy: 0.9854\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9934\n",
      "Epoch 00141: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 813ms/step - loss: 0.0266 - accuracy: 0.9934 - val_loss: 0.1651 - val_accuracy: 0.9801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9920\n",
      "Epoch 00142: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 798ms/step - loss: 0.0345 - accuracy: 0.9920 - val_loss: 0.1255 - val_accuracy: 0.9812\n",
      "Epoch 143/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9906\n",
      "Epoch 00143: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 16s 796ms/step - loss: 0.0391 - accuracy: 0.9906 - val_loss: 0.0968 - val_accuracy: 0.9800\n",
      "Epoch 144/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9891\n",
      "Epoch 00144: val_accuracy did not improve from 0.98726\n",
      "20/20 [==============================] - 17s 837ms/step - loss: 0.0483 - accuracy: 0.9891 - val_loss: 0.0746 - val_accuracy: 0.9842\n",
      "Epoch 145/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9923\n",
      "Epoch 00145: val_accuracy improved from 0.98726 to 0.98762, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 17s 827ms/step - loss: 0.0319 - accuracy: 0.9923 - val_loss: 0.0778 - val_accuracy: 0.9876\n",
      "Epoch 146/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9949\n",
      "Epoch 00146: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 816ms/step - loss: 0.0235 - accuracy: 0.9949 - val_loss: 0.0890 - val_accuracy: 0.9839\n",
      "Epoch 147/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9942\n",
      "Epoch 00147: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 821ms/step - loss: 0.0231 - accuracy: 0.9942 - val_loss: 0.0732 - val_accuracy: 0.9848\n",
      "Epoch 148/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9921\n",
      "Epoch 00148: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 798ms/step - loss: 0.0290 - accuracy: 0.9921 - val_loss: 0.0895 - val_accuracy: 0.9842\n",
      "Epoch 149/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9918\n",
      "Epoch 00149: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 811ms/step - loss: 0.0313 - accuracy: 0.9918 - val_loss: 0.1229 - val_accuracy: 0.9805\n",
      "Epoch 150/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9944\n",
      "Epoch 00150: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 780ms/step - loss: 0.0235 - accuracy: 0.9944 - val_loss: 0.0803 - val_accuracy: 0.9851\n",
      "Epoch 151/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9948\n",
      "Epoch 00151: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 17s 838ms/step - loss: 0.0229 - accuracy: 0.9948 - val_loss: 0.1157 - val_accuracy: 0.9860\n",
      "Epoch 152/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9936\n",
      "Epoch 00152: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 788ms/step - loss: 0.0287 - accuracy: 0.9936 - val_loss: 0.0723 - val_accuracy: 0.9874\n",
      "Epoch 153/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9929\n",
      "Epoch 00153: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 819ms/step - loss: 0.0324 - accuracy: 0.9929 - val_loss: 0.0838 - val_accuracy: 0.9855\n",
      "Epoch 154/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9866\n",
      "Epoch 00154: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 793ms/step - loss: 0.0570 - accuracy: 0.9866 - val_loss: 0.1063 - val_accuracy: 0.9805\n",
      "Epoch 155/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9873\n",
      "Epoch 00155: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 788ms/step - loss: 0.0612 - accuracy: 0.9873 - val_loss: 0.0895 - val_accuracy: 0.9826\n",
      "Epoch 156/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9898\n",
      "Epoch 00156: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 17s 828ms/step - loss: 0.0455 - accuracy: 0.9898 - val_loss: 0.0969 - val_accuracy: 0.9807\n",
      "Epoch 157/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9917\n",
      "Epoch 00157: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 811ms/step - loss: 0.0342 - accuracy: 0.9917 - val_loss: 0.1151 - val_accuracy: 0.9871\n",
      "Epoch 158/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9907\n",
      "Epoch 00158: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 815ms/step - loss: 0.0413 - accuracy: 0.9907 - val_loss: 0.0864 - val_accuracy: 0.9844\n",
      "Epoch 159/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9907\n",
      "Epoch 00159: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 17s 837ms/step - loss: 0.0459 - accuracy: 0.9907 - val_loss: 0.0922 - val_accuracy: 0.9818\n",
      "Epoch 160/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9897\n",
      "Epoch 00160: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 17s 840ms/step - loss: 0.0424 - accuracy: 0.9897 - val_loss: 0.0745 - val_accuracy: 0.9857\n",
      "Epoch 161/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9936\n",
      "Epoch 00161: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 17s 841ms/step - loss: 0.0298 - accuracy: 0.9936 - val_loss: 0.0657 - val_accuracy: 0.9858\n",
      "Epoch 162/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9946\n",
      "Epoch 00162: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 809ms/step - loss: 0.0228 - accuracy: 0.9946 - val_loss: 0.0924 - val_accuracy: 0.9873\n",
      "Epoch 163/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9944\n",
      "Epoch 00163: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 17s 862ms/step - loss: 0.0247 - accuracy: 0.9944 - val_loss: 0.0648 - val_accuracy: 0.9870\n",
      "Epoch 164/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9938\n",
      "Epoch 00164: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 15s 769ms/step - loss: 0.0240 - accuracy: 0.9938 - val_loss: 0.0900 - val_accuracy: 0.9837\n",
      "Epoch 165/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9950\n",
      "Epoch 00165: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 804ms/step - loss: 0.0187 - accuracy: 0.9950 - val_loss: 0.0769 - val_accuracy: 0.9869\n",
      "Epoch 166/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9952\n",
      "Epoch 00166: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 17s 831ms/step - loss: 0.0192 - accuracy: 0.9952 - val_loss: 0.0786 - val_accuracy: 0.9857\n",
      "Epoch 167/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9935\n",
      "Epoch 00167: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 822ms/step - loss: 0.0250 - accuracy: 0.9935 - val_loss: 0.1011 - val_accuracy: 0.9835\n",
      "Epoch 168/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9949\n",
      "Epoch 00168: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 802ms/step - loss: 0.0219 - accuracy: 0.9949 - val_loss: 0.0899 - val_accuracy: 0.9867\n",
      "Epoch 169/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9919\n",
      "Epoch 00169: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 794ms/step - loss: 0.0330 - accuracy: 0.9919 - val_loss: 0.1306 - val_accuracy: 0.9819\n",
      "Epoch 170/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9931\n",
      "Epoch 00170: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 801ms/step - loss: 0.0284 - accuracy: 0.9931 - val_loss: 0.1008 - val_accuracy: 0.9815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9921\n",
      "Epoch 00171: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 806ms/step - loss: 0.0315 - accuracy: 0.9921 - val_loss: 0.0872 - val_accuracy: 0.9854\n",
      "Epoch 172/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9933\n",
      "Epoch 00172: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 17s 842ms/step - loss: 0.0283 - accuracy: 0.9933 - val_loss: 0.0718 - val_accuracy: 0.9874\n",
      "Epoch 173/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9956\n",
      "Epoch 00173: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 794ms/step - loss: 0.0223 - accuracy: 0.9956 - val_loss: 0.0996 - val_accuracy: 0.9840\n",
      "Epoch 174/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9954\n",
      "Epoch 00174: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 814ms/step - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.0876 - val_accuracy: 0.9851\n",
      "Epoch 175/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9941\n",
      "Epoch 00175: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 790ms/step - loss: 0.0245 - accuracy: 0.9941 - val_loss: 0.1070 - val_accuracy: 0.9821\n",
      "Epoch 176/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9919\n",
      "Epoch 00176: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 801ms/step - loss: 0.0436 - accuracy: 0.9919 - val_loss: 0.1212 - val_accuracy: 0.9795\n",
      "Epoch 177/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0579 - accuracy: 0.9866\n",
      "Epoch 00177: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 786ms/step - loss: 0.0579 - accuracy: 0.9866 - val_loss: 0.1265 - val_accuracy: 0.9783\n",
      "Epoch 178/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9907\n",
      "Epoch 00178: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 17s 829ms/step - loss: 0.0388 - accuracy: 0.9907 - val_loss: 0.1512 - val_accuracy: 0.9823\n",
      "Epoch 179/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9881\n",
      "Epoch 00179: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 813ms/step - loss: 0.0566 - accuracy: 0.9881 - val_loss: 0.1493 - val_accuracy: 0.9756\n",
      "Epoch 180/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9895\n",
      "Epoch 00180: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 17s 833ms/step - loss: 0.0510 - accuracy: 0.9895 - val_loss: 0.1327 - val_accuracy: 0.9794\n",
      "Epoch 181/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9918\n",
      "Epoch 00181: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 809ms/step - loss: 0.0398 - accuracy: 0.9918 - val_loss: 0.0982 - val_accuracy: 0.9806\n",
      "Epoch 182/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9904\n",
      "Epoch 00182: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 804ms/step - loss: 0.0462 - accuracy: 0.9904 - val_loss: 0.1686 - val_accuracy: 0.9717\n",
      "Epoch 183/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1585 - accuracy: 0.9716\n",
      "Epoch 00183: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 798ms/step - loss: 0.1585 - accuracy: 0.9716 - val_loss: 0.2154 - val_accuracy: 0.9592\n",
      "Epoch 184/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9697\n",
      "Epoch 00184: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 794ms/step - loss: 0.1571 - accuracy: 0.9697 - val_loss: 0.1518 - val_accuracy: 0.9677\n",
      "Epoch 185/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1218 - accuracy: 0.9780\n",
      "Epoch 00185: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 810ms/step - loss: 0.1218 - accuracy: 0.9780 - val_loss: 0.1196 - val_accuracy: 0.9762\n",
      "Epoch 186/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9866\n",
      "Epoch 00186: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 811ms/step - loss: 0.0675 - accuracy: 0.9866 - val_loss: 0.1117 - val_accuracy: 0.9760\n",
      "Epoch 187/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9869\n",
      "Epoch 00187: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 795ms/step - loss: 0.0617 - accuracy: 0.9869 - val_loss: 0.1055 - val_accuracy: 0.9815\n",
      "Epoch 188/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9922\n",
      "Epoch 00188: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 794ms/step - loss: 0.0360 - accuracy: 0.9922 - val_loss: 0.0742 - val_accuracy: 0.9855\n",
      "Epoch 189/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9902\n",
      "Epoch 00189: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 824ms/step - loss: 0.0468 - accuracy: 0.9902 - val_loss: 0.1039 - val_accuracy: 0.9812\n",
      "Epoch 190/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9906\n",
      "Epoch 00190: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 809ms/step - loss: 0.0458 - accuracy: 0.9906 - val_loss: 0.0843 - val_accuracy: 0.9855\n",
      "Epoch 191/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9921\n",
      "Epoch 00191: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 802ms/step - loss: 0.0371 - accuracy: 0.9921 - val_loss: 0.0847 - val_accuracy: 0.9830\n",
      "Epoch 192/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9921\n",
      "Epoch 00192: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 798ms/step - loss: 0.0368 - accuracy: 0.9921 - val_loss: 0.0817 - val_accuracy: 0.9836\n",
      "Epoch 193/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9929\n",
      "Epoch 00193: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 824ms/step - loss: 0.0281 - accuracy: 0.9929 - val_loss: 0.0908 - val_accuracy: 0.9856\n",
      "Epoch 194/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9953\n",
      "Epoch 00194: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 778ms/step - loss: 0.0224 - accuracy: 0.9953 - val_loss: 0.0941 - val_accuracy: 0.9848\n",
      "Epoch 195/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9937\n",
      "Epoch 00195: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 808ms/step - loss: 0.0264 - accuracy: 0.9937 - val_loss: 0.0812 - val_accuracy: 0.9871\n",
      "Epoch 196/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9956\n",
      "Epoch 00196: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 16s 792ms/step - loss: 0.0215 - accuracy: 0.9956 - val_loss: 0.0727 - val_accuracy: 0.9868\n",
      "Epoch 197/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9969\n",
      "Epoch 00197: val_accuracy did not improve from 0.98762\n",
      "20/20 [==============================] - 17s 834ms/step - loss: 0.0133 - accuracy: 0.9969 - val_loss: 0.0899 - val_accuracy: 0.9852\n",
      "Epoch 198/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9942\n",
      "Epoch 00198: val_accuracy improved from 0.98762 to 0.98786, saving model to VGG16_modif_best.h5\n",
      "20/20 [==============================] - 16s 816ms/step - loss: 0.0269 - accuracy: 0.9942 - val_loss: 0.0906 - val_accuracy: 0.9879\n",
      "Epoch 199/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9948\n",
      "Epoch 00199: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 792ms/step - loss: 0.0233 - accuracy: 0.9948 - val_loss: 0.1050 - val_accuracy: 0.9852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9958\n",
      "Epoch 00200: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 838ms/step - loss: 0.0164 - accuracy: 0.9958 - val_loss: 0.0900 - val_accuracy: 0.9867\n",
      "Epoch 201/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9903\n",
      "Epoch 00201: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 852ms/step - loss: 0.0577 - accuracy: 0.9903 - val_loss: 0.1102 - val_accuracy: 0.9787\n",
      "Epoch 202/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9873\n",
      "Epoch 00202: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 821ms/step - loss: 0.0572 - accuracy: 0.9873 - val_loss: 0.0841 - val_accuracy: 0.9863\n",
      "Epoch 203/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9918\n",
      "Epoch 00203: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 819ms/step - loss: 0.0379 - accuracy: 0.9918 - val_loss: 0.1106 - val_accuracy: 0.9769\n",
      "Epoch 204/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9869\n",
      "Epoch 00204: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 786ms/step - loss: 0.0564 - accuracy: 0.9869 - val_loss: 0.0964 - val_accuracy: 0.9785\n",
      "Epoch 205/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9924\n",
      "Epoch 00205: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 826ms/step - loss: 0.0291 - accuracy: 0.9924 - val_loss: 0.1000 - val_accuracy: 0.9854\n",
      "Epoch 206/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9938\n",
      "Epoch 00206: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 805ms/step - loss: 0.0295 - accuracy: 0.9938 - val_loss: 0.0822 - val_accuracy: 0.9852\n",
      "Epoch 207/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9957\n",
      "Epoch 00207: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 844ms/step - loss: 0.0160 - accuracy: 0.9957 - val_loss: 0.0961 - val_accuracy: 0.9867\n",
      "Epoch 208/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9928\n",
      "Epoch 00208: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 791ms/step - loss: 0.0333 - accuracy: 0.9928 - val_loss: 0.1066 - val_accuracy: 0.9800\n",
      "Epoch 209/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9939\n",
      "Epoch 00209: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 806ms/step - loss: 0.0241 - accuracy: 0.9939 - val_loss: 0.0742 - val_accuracy: 0.9870\n",
      "Epoch 210/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9972\n",
      "Epoch 00210: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 827ms/step - loss: 0.0132 - accuracy: 0.9972 - val_loss: 0.0891 - val_accuracy: 0.9871\n",
      "Epoch 211/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9957\n",
      "Epoch 00211: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 15s 774ms/step - loss: 0.0161 - accuracy: 0.9957 - val_loss: 0.0939 - val_accuracy: 0.9855\n",
      "Epoch 212/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9948\n",
      "Epoch 00212: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 801ms/step - loss: 0.0225 - accuracy: 0.9948 - val_loss: 0.1145 - val_accuracy: 0.9855\n",
      "Epoch 213/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9909\n",
      "Epoch 00213: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 797ms/step - loss: 0.0407 - accuracy: 0.9909 - val_loss: 0.1029 - val_accuracy: 0.9842\n",
      "Epoch 214/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9883\n",
      "Epoch 00214: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 834ms/step - loss: 0.0479 - accuracy: 0.9883 - val_loss: 0.1123 - val_accuracy: 0.9807\n",
      "Epoch 215/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9933\n",
      "Epoch 00215: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 801ms/step - loss: 0.0362 - accuracy: 0.9933 - val_loss: 0.1096 - val_accuracy: 0.9840\n",
      "Epoch 216/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1035 - accuracy: 0.9787\n",
      "Epoch 00216: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 819ms/step - loss: 0.1035 - accuracy: 0.9787 - val_loss: 0.1599 - val_accuracy: 0.9704\n",
      "Epoch 217/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9844\n",
      "Epoch 00217: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 801ms/step - loss: 0.0789 - accuracy: 0.9844 - val_loss: 0.1057 - val_accuracy: 0.9813\n",
      "Epoch 218/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9920\n",
      "Epoch 00218: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 812ms/step - loss: 0.0309 - accuracy: 0.9920 - val_loss: 0.0871 - val_accuracy: 0.9860\n",
      "Epoch 219/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9868\n",
      "Epoch 00219: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 809ms/step - loss: 0.0590 - accuracy: 0.9868 - val_loss: 0.1095 - val_accuracy: 0.9831\n",
      "Epoch 220/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9906\n",
      "Epoch 00220: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 843ms/step - loss: 0.0414 - accuracy: 0.9906 - val_loss: 0.0728 - val_accuracy: 0.9848\n",
      "Epoch 221/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9924\n",
      "Epoch 00221: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 829ms/step - loss: 0.0312 - accuracy: 0.9924 - val_loss: 0.1040 - val_accuracy: 0.9839\n",
      "Epoch 222/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9877\n",
      "Epoch 00222: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 810ms/step - loss: 0.0547 - accuracy: 0.9877 - val_loss: 0.1166 - val_accuracy: 0.9827\n",
      "Epoch 223/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9918\n",
      "Epoch 00223: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 793ms/step - loss: 0.0395 - accuracy: 0.9918 - val_loss: 0.1441 - val_accuracy: 0.9800\n",
      "Epoch 224/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0908 - accuracy: 0.9857\n",
      "Epoch 00224: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 827ms/step - loss: 0.0908 - accuracy: 0.9857 - val_loss: 0.3234 - val_accuracy: 0.9792\n",
      "Epoch 225/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.2463 - accuracy: 0.9903\n",
      "Epoch 00225: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 785ms/step - loss: 0.2463 - accuracy: 0.9903 - val_loss: 0.3027 - val_accuracy: 0.9836\n",
      "Epoch 226/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.9906\n",
      "Epoch 00226: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 833ms/step - loss: 0.1852 - accuracy: 0.9906 - val_loss: 0.4664 - val_accuracy: 0.9413\n",
      "Epoch 227/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1902 - accuracy: 0.9646\n",
      "Epoch 00227: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 801ms/step - loss: 0.1902 - accuracy: 0.9646 - val_loss: 0.2238 - val_accuracy: 0.9649\n",
      "Epoch 228/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1360 - accuracy: 0.9727\n",
      "Epoch 00228: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 835ms/step - loss: 0.1360 - accuracy: 0.9727 - val_loss: 0.0958 - val_accuracy: 0.9780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9872\n",
      "Epoch 00229: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 790ms/step - loss: 0.0764 - accuracy: 0.9872 - val_loss: 0.1977 - val_accuracy: 0.9749\n",
      "Epoch 230/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.2615 - accuracy: 0.9759\n",
      "Epoch 00230: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 801ms/step - loss: 0.2615 - accuracy: 0.9759 - val_loss: 0.3300 - val_accuracy: 0.9317\n",
      "Epoch 231/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.2377 - accuracy: 0.9811\n",
      "Epoch 00231: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 789ms/step - loss: 0.2377 - accuracy: 0.9811 - val_loss: 0.2746 - val_accuracy: 0.9811\n",
      "Epoch 232/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.2338 - accuracy: 0.9837\n",
      "Epoch 00232: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 811ms/step - loss: 0.2338 - accuracy: 0.9837 - val_loss: 0.3148 - val_accuracy: 0.9800\n",
      "Epoch 233/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.2009 - accuracy: 0.9899\n",
      "Epoch 00233: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 796ms/step - loss: 0.2009 - accuracy: 0.9899 - val_loss: 0.2373 - val_accuracy: 0.9851\n",
      "Epoch 234/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1814 - accuracy: 0.9926\n",
      "Epoch 00234: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 822ms/step - loss: 0.1814 - accuracy: 0.9926 - val_loss: 0.2156 - val_accuracy: 0.9865\n",
      "Epoch 235/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1604 - accuracy: 0.9946\n",
      "Epoch 00235: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 787ms/step - loss: 0.1604 - accuracy: 0.9946 - val_loss: 0.2245 - val_accuracy: 0.9857\n",
      "Epoch 236/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.9934\n",
      "Epoch 00236: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 846ms/step - loss: 0.1621 - accuracy: 0.9934 - val_loss: 0.2256 - val_accuracy: 0.9849\n",
      "Epoch 237/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1498 - accuracy: 0.9945\n",
      "Epoch 00237: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 796ms/step - loss: 0.1498 - accuracy: 0.9945 - val_loss: 0.2034 - val_accuracy: 0.9860\n",
      "Epoch 238/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1404 - accuracy: 0.9954\n",
      "Epoch 00238: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 800ms/step - loss: 0.1404 - accuracy: 0.9954 - val_loss: 0.2070 - val_accuracy: 0.9863\n",
      "Epoch 239/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1360 - accuracy: 0.9959\n",
      "Epoch 00239: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 825ms/step - loss: 0.1360 - accuracy: 0.9959 - val_loss: 0.1903 - val_accuracy: 0.9874\n",
      "Epoch 240/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1282 - accuracy: 0.9963\n",
      "Epoch 00240: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 783ms/step - loss: 0.1282 - accuracy: 0.9963 - val_loss: 0.2300 - val_accuracy: 0.9858\n",
      "Epoch 241/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9963\n",
      "Epoch 00241: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 800ms/step - loss: 0.1247 - accuracy: 0.9963 - val_loss: 0.2108 - val_accuracy: 0.9846\n",
      "Epoch 242/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1189 - accuracy: 0.9969\n",
      "Epoch 00242: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 837ms/step - loss: 0.1189 - accuracy: 0.9969 - val_loss: 0.1834 - val_accuracy: 0.9875\n",
      "Epoch 243/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.9959\n",
      "Epoch 00243: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 819ms/step - loss: 0.1145 - accuracy: 0.9959 - val_loss: 0.1758 - val_accuracy: 0.9874\n",
      "Epoch 244/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0969 - accuracy: 0.9962\n",
      "Epoch 00244: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 807ms/step - loss: 0.0969 - accuracy: 0.9962 - val_loss: 0.1500 - val_accuracy: 0.9865\n",
      "Epoch 245/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9914\n",
      "Epoch 00245: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 15s 771ms/step - loss: 0.0576 - accuracy: 0.9914 - val_loss: 0.1090 - val_accuracy: 0.9799\n",
      "Epoch 246/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9899\n",
      "Epoch 00246: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 832ms/step - loss: 0.0564 - accuracy: 0.9899 - val_loss: 0.1506 - val_accuracy: 0.9802\n",
      "Epoch 247/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9899\n",
      "Epoch 00247: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 801ms/step - loss: 0.0510 - accuracy: 0.9899 - val_loss: 0.1051 - val_accuracy: 0.9845\n",
      "Epoch 248/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9927\n",
      "Epoch 00248: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 807ms/step - loss: 0.0396 - accuracy: 0.9927 - val_loss: 0.1147 - val_accuracy: 0.9827\n",
      "Epoch 249/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9902\n",
      "Epoch 00249: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 15s 737ms/step - loss: 0.0551 - accuracy: 0.9902 - val_loss: 0.1809 - val_accuracy: 0.9819\n",
      "Epoch 250/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9860\n",
      "Epoch 00250: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 828ms/step - loss: 0.0765 - accuracy: 0.9860 - val_loss: 0.1609 - val_accuracy: 0.9704\n",
      "Epoch 251/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0824 - accuracy: 0.9796\n",
      "Epoch 00251: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 836ms/step - loss: 0.0824 - accuracy: 0.9796 - val_loss: 0.1195 - val_accuracy: 0.9805\n",
      "Epoch 252/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.3021 - accuracy: 0.9742\n",
      "Epoch 00252: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 15s 774ms/step - loss: 0.3021 - accuracy: 0.9742 - val_loss: 0.1995 - val_accuracy: 0.9620\n",
      "Epoch 253/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9825\n",
      "Epoch 00253: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 806ms/step - loss: 0.0875 - accuracy: 0.9825 - val_loss: 0.1370 - val_accuracy: 0.9762\n",
      "Epoch 254/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9884\n",
      "Epoch 00254: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 815ms/step - loss: 0.0547 - accuracy: 0.9884 - val_loss: 0.0907 - val_accuracy: 0.9838\n",
      "Epoch 255/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9910\n",
      "Epoch 00255: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 15s 774ms/step - loss: 0.0399 - accuracy: 0.9910 - val_loss: 0.0896 - val_accuracy: 0.9855\n",
      "Epoch 256/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9935\n",
      "Epoch 00256: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 842ms/step - loss: 0.0273 - accuracy: 0.9935 - val_loss: 0.0951 - val_accuracy: 0.9865\n",
      "Epoch 257/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9899\n",
      "Epoch 00257: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 833ms/step - loss: 0.0457 - accuracy: 0.9899 - val_loss: 0.1020 - val_accuracy: 0.9826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 258/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9924\n",
      "Epoch 00258: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 817ms/step - loss: 0.0391 - accuracy: 0.9924 - val_loss: 0.1102 - val_accuracy: 0.9842\n",
      "Epoch 259/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9934\n",
      "Epoch 00259: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 783ms/step - loss: 0.0262 - accuracy: 0.9934 - val_loss: 0.0958 - val_accuracy: 0.9879\n",
      "Epoch 260/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9954\n",
      "Epoch 00260: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 842ms/step - loss: 0.0209 - accuracy: 0.9954 - val_loss: 0.0886 - val_accuracy: 0.9862\n",
      "Epoch 261/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9956\n",
      "Epoch 00261: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 820ms/step - loss: 0.0171 - accuracy: 0.9956 - val_loss: 0.0981 - val_accuracy: 0.9858\n",
      "Epoch 262/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9965\n",
      "Epoch 00262: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 796ms/step - loss: 0.0149 - accuracy: 0.9965 - val_loss: 0.1256 - val_accuracy: 0.9861\n",
      "Epoch 263/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9970\n",
      "Epoch 00263: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 822ms/step - loss: 0.0129 - accuracy: 0.9970 - val_loss: 0.1002 - val_accuracy: 0.9864\n",
      "Epoch 264/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9973\n",
      "Epoch 00264: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 817ms/step - loss: 0.0095 - accuracy: 0.9973 - val_loss: 0.1168 - val_accuracy: 0.9874\n",
      "Epoch 265/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9905\n",
      "Epoch 00265: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 808ms/step - loss: 0.0670 - accuracy: 0.9905 - val_loss: 0.1255 - val_accuracy: 0.9806\n",
      "Epoch 266/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1022 - accuracy: 0.9751\n",
      "Epoch 00266: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 821ms/step - loss: 0.1022 - accuracy: 0.9751 - val_loss: 0.2244 - val_accuracy: 0.9777\n",
      "Epoch 267/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.3056 - accuracy: 0.9356\n",
      "Epoch 00267: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 791ms/step - loss: 0.3056 - accuracy: 0.9356 - val_loss: 11.3053 - val_accuracy: 0.8627\n",
      "Epoch 268/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.0006 - accuracy: 0.8824\n",
      "Epoch 00268: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 829ms/step - loss: 1.0006 - accuracy: 0.8824 - val_loss: 0.2744 - val_accuracy: 0.9433\n",
      "Epoch 269/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.1719 - accuracy: 0.9582\n",
      "Epoch 00269: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 15s 769ms/step - loss: 0.1719 - accuracy: 0.9582 - val_loss: 0.1515 - val_accuracy: 0.9682\n",
      "Epoch 270/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9778\n",
      "Epoch 00270: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 809ms/step - loss: 0.0928 - accuracy: 0.9778 - val_loss: 0.1115 - val_accuracy: 0.9757\n",
      "Epoch 271/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9830\n",
      "Epoch 00271: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 800ms/step - loss: 0.0729 - accuracy: 0.9830 - val_loss: 0.0997 - val_accuracy: 0.9776\n",
      "Epoch 272/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9845\n",
      "Epoch 00272: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 18s 900ms/step - loss: 0.0613 - accuracy: 0.9845 - val_loss: 0.0869 - val_accuracy: 0.9817\n",
      "Epoch 273/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9890\n",
      "Epoch 00273: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 806ms/step - loss: 0.0445 - accuracy: 0.9890 - val_loss: 0.0877 - val_accuracy: 0.9818\n",
      "Epoch 274/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9903\n",
      "Epoch 00274: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 816ms/step - loss: 0.0397 - accuracy: 0.9903 - val_loss: 0.0849 - val_accuracy: 0.9829\n",
      "Epoch 275/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9909\n",
      "Epoch 00275: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 791ms/step - loss: 0.0351 - accuracy: 0.9909 - val_loss: 0.0832 - val_accuracy: 0.9838\n",
      "Epoch 276/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9914\n",
      "Epoch 00276: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 822ms/step - loss: 0.0295 - accuracy: 0.9914 - val_loss: 0.0855 - val_accuracy: 0.9835\n",
      "Epoch 277/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9930\n",
      "Epoch 00277: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 821ms/step - loss: 0.0274 - accuracy: 0.9930 - val_loss: 0.0814 - val_accuracy: 0.9835\n",
      "Epoch 278/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9931\n",
      "Epoch 00278: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 854ms/step - loss: 0.0286 - accuracy: 0.9931 - val_loss: 0.0836 - val_accuracy: 0.9843\n",
      "Epoch 279/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9915\n",
      "Epoch 00279: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 799ms/step - loss: 0.0304 - accuracy: 0.9915 - val_loss: 0.0734 - val_accuracy: 0.9842\n",
      "Epoch 280/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9941\n",
      "Epoch 00280: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 792ms/step - loss: 0.0220 - accuracy: 0.9941 - val_loss: 0.0731 - val_accuracy: 0.9860\n",
      "Epoch 281/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9961\n",
      "Epoch 00281: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 811ms/step - loss: 0.0162 - accuracy: 0.9961 - val_loss: 0.0765 - val_accuracy: 0.9869\n",
      "Epoch 282/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9952\n",
      "Epoch 00282: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 814ms/step - loss: 0.0169 - accuracy: 0.9952 - val_loss: 0.0863 - val_accuracy: 0.9850\n",
      "Epoch 283/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9951\n",
      "Epoch 00283: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 779ms/step - loss: 0.0187 - accuracy: 0.9951 - val_loss: 0.0773 - val_accuracy: 0.9858\n",
      "Epoch 284/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9947\n",
      "Epoch 00284: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 799ms/step - loss: 0.0218 - accuracy: 0.9947 - val_loss: 0.0919 - val_accuracy: 0.9850\n",
      "Epoch 285/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9958\n",
      "Epoch 00285: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 831ms/step - loss: 0.0169 - accuracy: 0.9958 - val_loss: 0.0763 - val_accuracy: 0.9867\n",
      "Epoch 286/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9964\n",
      "Epoch 00286: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 795ms/step - loss: 0.0140 - accuracy: 0.9964 - val_loss: 0.0807 - val_accuracy: 0.9865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 287/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9966\n",
      "Epoch 00287: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 801ms/step - loss: 0.0118 - accuracy: 0.9966 - val_loss: 0.0892 - val_accuracy: 0.9857\n",
      "Epoch 288/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9970\n",
      "Epoch 00288: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 838ms/step - loss: 0.0122 - accuracy: 0.9970 - val_loss: 0.0897 - val_accuracy: 0.9862\n",
      "Epoch 289/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9968\n",
      "Epoch 00289: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 780ms/step - loss: 0.0111 - accuracy: 0.9968 - val_loss: 0.0835 - val_accuracy: 0.9873\n",
      "Epoch 290/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.9977\n",
      "Epoch 00290: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 840ms/step - loss: 0.0094 - accuracy: 0.9977 - val_loss: 0.0998 - val_accuracy: 0.9860\n",
      "Epoch 291/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 0.9971\n",
      "Epoch 00291: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 822ms/step - loss: 0.0105 - accuracy: 0.9971 - val_loss: 0.0821 - val_accuracy: 0.9863\n",
      "Epoch 292/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9967\n",
      "Epoch 00292: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 821ms/step - loss: 0.0119 - accuracy: 0.9967 - val_loss: 0.0915 - val_accuracy: 0.9875\n",
      "Epoch 293/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9964\n",
      "Epoch 00293: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 819ms/step - loss: 0.0157 - accuracy: 0.9964 - val_loss: 0.0835 - val_accuracy: 0.9869\n",
      "Epoch 294/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9966\n",
      "Epoch 00294: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 822ms/step - loss: 0.0110 - accuracy: 0.9966 - val_loss: 0.0868 - val_accuracy: 0.9868\n",
      "Epoch 295/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9935\n",
      "Epoch 00295: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 783ms/step - loss: 0.0214 - accuracy: 0.9935 - val_loss: 0.1665 - val_accuracy: 0.9749\n",
      "Epoch 296/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9952\n",
      "Epoch 00296: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 810ms/step - loss: 0.0220 - accuracy: 0.9952 - val_loss: 0.1482 - val_accuracy: 0.9830\n",
      "Epoch 297/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9942\n",
      "Epoch 00297: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 806ms/step - loss: 0.0277 - accuracy: 0.9942 - val_loss: 0.1161 - val_accuracy: 0.9852\n",
      "Epoch 298/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9934\n",
      "Epoch 00298: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 794ms/step - loss: 0.0270 - accuracy: 0.9934 - val_loss: 0.0986 - val_accuracy: 0.9852\n",
      "Epoch 299/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9936\n",
      "Epoch 00299: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 842ms/step - loss: 0.0280 - accuracy: 0.9936 - val_loss: 0.0963 - val_accuracy: 0.9864\n",
      "Epoch 300/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9948\n",
      "Epoch 00300: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 805ms/step - loss: 0.0218 - accuracy: 0.9948 - val_loss: 0.1120 - val_accuracy: 0.9844\n",
      "Epoch 301/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9939\n",
      "Epoch 00301: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 786ms/step - loss: 0.0288 - accuracy: 0.9939 - val_loss: 0.1219 - val_accuracy: 0.9839\n",
      "Epoch 302/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9899\n",
      "Epoch 00302: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 833ms/step - loss: 0.0489 - accuracy: 0.9899 - val_loss: 0.1288 - val_accuracy: 0.9795\n",
      "Epoch 303/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9943\n",
      "Epoch 00303: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 805ms/step - loss: 0.0207 - accuracy: 0.9943 - val_loss: 0.1196 - val_accuracy: 0.9846\n",
      "Epoch 304/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9941\n",
      "Epoch 00304: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 789ms/step - loss: 0.0302 - accuracy: 0.9941 - val_loss: 0.0942 - val_accuracy: 0.9832\n",
      "Epoch 305/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9942\n",
      "Epoch 00305: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 798ms/step - loss: 0.0222 - accuracy: 0.9942 - val_loss: 0.1203 - val_accuracy: 0.9850\n",
      "Epoch 306/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9933\n",
      "Epoch 00306: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 811ms/step - loss: 0.0283 - accuracy: 0.9933 - val_loss: 0.1033 - val_accuracy: 0.9857\n",
      "Epoch 307/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9941\n",
      "Epoch 00307: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 828ms/step - loss: 0.0268 - accuracy: 0.9941 - val_loss: 0.1238 - val_accuracy: 0.9837\n",
      "Epoch 308/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9944\n",
      "Epoch 00308: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 803ms/step - loss: 0.0223 - accuracy: 0.9944 - val_loss: 0.0920 - val_accuracy: 0.9842\n",
      "Epoch 309/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9965\n",
      "Epoch 00309: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 806ms/step - loss: 0.0162 - accuracy: 0.9965 - val_loss: 0.1003 - val_accuracy: 0.9865\n",
      "Epoch 310/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.9970\n",
      "Epoch 00310: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 820ms/step - loss: 0.0097 - accuracy: 0.9970 - val_loss: 0.0883 - val_accuracy: 0.9862\n",
      "Epoch 311/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9977\n",
      "Epoch 00311: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 823ms/step - loss: 0.0086 - accuracy: 0.9977 - val_loss: 0.1095 - val_accuracy: 0.9877\n",
      "Epoch 312/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 0.9983\n",
      "Epoch 00312: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 781ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.1343 - val_accuracy: 0.9879\n",
      "Epoch 313/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9970\n",
      "Epoch 00313: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 850ms/step - loss: 0.0115 - accuracy: 0.9970 - val_loss: 0.0956 - val_accuracy: 0.9864\n",
      "Epoch 314/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9950\n",
      "Epoch 00314: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 812ms/step - loss: 0.0209 - accuracy: 0.9950 - val_loss: 0.1161 - val_accuracy: 0.9833\n",
      "Epoch 315/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9880\n",
      "Epoch 00315: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 17s 842ms/step - loss: 0.0566 - accuracy: 0.9880 - val_loss: 0.1100 - val_accuracy: 0.9826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 316/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.9815\n",
      "Epoch 00316: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 787ms/step - loss: 0.0896 - accuracy: 0.9815 - val_loss: 0.1131 - val_accuracy: 0.9800\n",
      "Epoch 317/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9870\n",
      "Epoch 00317: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 819ms/step - loss: 0.0700 - accuracy: 0.9870 - val_loss: 0.1785 - val_accuracy: 0.9746\n",
      "Epoch 318/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0845 - accuracy: 0.9830\n",
      "Epoch 00318: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 818ms/step - loss: 0.0845 - accuracy: 0.9830 - val_loss: 0.1254 - val_accuracy: 0.9805\n",
      "Epoch 319/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9895\n",
      "Epoch 00319: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 817ms/step - loss: 0.0490 - accuracy: 0.9895 - val_loss: 0.1380 - val_accuracy: 0.9781\n",
      "Epoch 320/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9903\n",
      "Epoch 00320: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 16s 800ms/step - loss: 0.0484 - accuracy: 0.9903 - val_loss: 0.1172 - val_accuracy: 0.9836\n",
      "Epoch 321/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9890\n",
      "Epoch 00321: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 19s 968ms/step - loss: 0.0510 - accuracy: 0.9890 - val_loss: 0.1128 - val_accuracy: 0.9840\n",
      "Epoch 322/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9913\n",
      "Epoch 00322: val_accuracy did not improve from 0.98786\n",
      "20/20 [==============================] - 19s 959ms/step - loss: 0.0383 - accuracy: 0.9913 - val_loss: 0.0982 - val_accuracy: 0.9854\n",
      "Epoch 323/1000\n",
      "14/20 [====================>.........] - ETA: 4s - loss: 0.0339 - accuracy: 0.9925"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-437cc5fe1c8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m hist = model.fit(X_train, y_train, validation_data= (X_valid, y_valid), steps_per_epoch=20,batch_size=1000,\n\u001b[0m\u001b[1;32m      2\u001b[0m                            epochs=1000,callbacks=[checkpoint])\n",
      "\u001b[0;32m~/anaconda3/envs/ironhack/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ironhack/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ironhack/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ironhack/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ironhack/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ironhack/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/envs/ironhack/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/ironhack/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ironhack/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train, validation_data= (X_valid, y_valid), steps_per_epoch=20,batch_size=1000,\n",
    "                           epochs=1000,callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('VGG16_modif_best.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows Ã— 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0           0       0       0       0       0       0       0       0       0   \n",
       "1           0       0       0       0       0       0       0       0       0   \n",
       "2           0       0       0       0       0       0       0       0       0   \n",
       "3           0       0       0       0       0       0       0       0       0   \n",
       "4           0       0       0       0       0       0       0       0       0   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "27995       0       0       0       0       0       0       0       0       0   \n",
       "27996       0       0       0       0       0       0       0       0       0   \n",
       "27997       0       0       0       0       0       0       0       0       0   \n",
       "27998       0       0       0       0       0       0       0       0       0   \n",
       "27999       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0           0  ...         0         0         0         0         0   \n",
       "1           0  ...         0         0         0         0         0   \n",
       "2           0  ...         0         0         0         0         0   \n",
       "3           0  ...         0         0         0         0         0   \n",
       "4           0  ...         0         0         0         0         0   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "27995       0  ...         0         0         0         0         0   \n",
       "27996       0  ...         0         0         0         0         0   \n",
       "27997       0  ...         0         0         0         0         0   \n",
       "27998       0  ...         0         0         0         0         0   \n",
       "27999       0  ...         0         0         0         0         0   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0             0         0         0         0         0  \n",
       "1             0         0         0         0         0  \n",
       "2             0         0         0         0         0  \n",
       "3             0         0         0         0         0  \n",
       "4             0         0         0         0         0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "27995         0         0         0         0         0  \n",
       "27996         0         0         0         0         0  \n",
       "27997         0         0         0         0         0  \n",
       "27998         0         0         0         0         0  \n",
       "27999         0         0         0         0         0  \n",
       "\n",
       "[28000 rows x 784 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test=pd.read_csv(\"test.csv\")\n",
    "raw_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = raw_test.values.reshape(28000, 28, 28)/ 255.0\n",
    "Xt = Xt.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_pred=np.argmax(model.predict(Xt), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=12\n",
    "plt.imshow(Xt[i]);\n",
    "print(ytest_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id=list(range(1,len(ytest_pred)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred=pd.DataFrame({\"ImageId\":img_id, \"Label\":ytest_pred})\n",
    "df_pred=df_pred.set_index('ImageId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.to_csv(\"iop_submision_VGG16modif.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
